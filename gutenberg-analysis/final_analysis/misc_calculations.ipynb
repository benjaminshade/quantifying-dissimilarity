{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains various calculations mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import pickle\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_1samp, ttest_rel\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from scipy import stats as stats\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "## path to the downloaded gutenberg corpus\n",
    "path_gutenberg = os.path.join(os.pardir,os.pardir,'gutenberg')\n",
    "\n",
    "## import internal helper functions\n",
    "src_dir = os.path.join(os.pardir,'src')\n",
    "sys.path.append(src_dir)\n",
    "from data_io import get_book\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Accessing the metadata\n",
    "sys.path.append(os.path.join(path_gutenberg,'src'))\n",
    "from metaquery import meta_query\n",
    "mq = meta_query(path=os.path.join(path_gutenberg,'metadata','metadata.csv'), filter_exist=False)\n",
    "\n",
    "from itertools import combinations\n",
    "from data_io import get_book, get_p12_same_support\n",
    "from metric_eval import prob_x_less_than_y_freq_new, prob_x_less_than_y_emb_new\n",
    "from jsd import jsdalpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying optimal values of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_se_dict(weights = False):\n",
    "    if weights:\n",
    "        string = '_weights'\n",
    "    else:\n",
    "        string = ''\n",
    "\n",
    "    dataframe_dict = {}\n",
    "    dataframe_dict_controlled = {}\n",
    "\n",
    "    task_numbers = [i for i in range(11,21)]\n",
    "    alphas = [i/20 for i in range(41)]\n",
    "    col_names = [str(i) for i in alphas]\n",
    "\n",
    "    for task in ['author', 'subject', 'time']:\n",
    "        dataframe_list = []\n",
    "        dataframe_list_controlled = []\n",
    "\n",
    "        for num in task_numbers:\n",
    "            input_file_path_opt_alpha_controlled = f'../output_files/optimal_alpha{string}_new_controlled_{task}{num}.pickle'\n",
    "            with open(input_file_path_opt_alpha_controlled, 'rb') as f:\n",
    "                opt_alpha_results_controlled = pickle.load(f)\n",
    "\n",
    "            input_file_path_opt_alpha = f'../output_files/optimal_alpha{string}_new_{task}{num}.pickle'\n",
    "            with open(input_file_path_opt_alpha, 'rb') as f:\n",
    "                opt_alpha_results = pickle.load(f)\n",
    "            \n",
    "            dataframe_list.append(opt_alpha_results.copy())\n",
    "            dataframe_list_controlled.append(opt_alpha_results_controlled.copy())\n",
    "        \n",
    "        df = pd.DataFrame(dataframe_list, columns=col_names)\n",
    "        df_controlled = pd.DataFrame(dataframe_list_controlled, columns=col_names)\n",
    "\n",
    "        alpha_avgs = []\n",
    "        alpha_stds = []\n",
    "\n",
    "        alpha_avgs_controlled = []\n",
    "        alpha_stds_controlled = []\n",
    "\n",
    "        for alpha in col_names:\n",
    "            current = df[alpha].to_numpy()\n",
    "            alpha_avgs.append(np.mean(current))\n",
    "            alpha_stds.append(np.std(current))\n",
    "\n",
    "            current_controlled = df_controlled[alpha].to_numpy()\n",
    "            alpha_avgs_controlled.append(np.mean(current_controlled))\n",
    "            alpha_stds_controlled.append(np.std(current_controlled))\n",
    "        \n",
    "        dataframe_dict[task] = (tuple(alpha_avgs), tuple(alpha_stds))\n",
    "        dataframe_dict_controlled[task] = (tuple(alpha_avgs_controlled), tuple(alpha_stds_controlled))\n",
    "    \n",
    "    return dataframe_dict_controlled, dataframe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncontrolled corpora\n",
      "author: P(X<Y) = 0.8742726, alpha = 0.65\n",
      "subject: P(X<Y) = 0.6749185, alpha = 0.6\n",
      "time: P(X<Y) = 0.5712482000000001, alpha = 0.8\n",
      "\n",
      "Controlled corpora\n",
      "author: P(X<Y) = 0.8960076000000001, alpha = 0.7\n",
      "subject: P(X<Y) = 0.6542960000000001, alpha = 0.6\n",
      "time: P(X<Y) = 0.6734418000000001, alpha = 0.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Uncontrolled corpora\")\n",
    "dataframe_dict_controlled, dataframe_dict = create_avg_se_dict()\n",
    "alphas = [i/20 for i in range(41)]\n",
    "task_numbers = [i for i in range(11,21)]\n",
    "for task in ['author', 'subject', 'time']:\n",
    "    vals = dataframe_dict[task][0]\n",
    "    index = vals.index(max(vals))\n",
    "    print(f'{task}: P(X<Y) = {vals[index]}, alpha = {alphas[index]}')\n",
    "\n",
    "\n",
    "print(\"\\nControlled corpora\")\n",
    "for task in ['author', 'subject', 'time']:\n",
    "    vals = dataframe_dict_controlled[task][0]\n",
    "    # index = alphas.index(1.0)\n",
    "    index = vals.index(max(vals))\n",
    "    print(f'{task}: P(X<Y) = {vals[index]}, alpha = {alphas[index]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of texts in our filtered corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform necessary filtering\n",
    "mq.reset()\n",
    "mq.filter_lang('en',how='only')\n",
    "# Only select books with more than 20 downloads\n",
    "df = mq.get_df()\n",
    "mq.df = df[df['downloads'] >= 20]\n",
    "# 1800 onwards\n",
    "mq.filter_year([1800, 2050])\n",
    "# Filter out data with no subject listed\n",
    "df = mq.get_df()\n",
    "mq.df = df[df['subjects'] != 'set()']\n",
    "# Filter out entries that don't have author birth or death year\n",
    "df = mq.get_df()\n",
    "mq.df = df[df[['authoryearofbirth', 'authoryearofdeath']].notnull().all(1)]\n",
    "\n",
    "all_uniq_ids = mq.get_ids()\n",
    "print(len(all_uniq_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary table of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing results\n",
    "# Columns: alpha=1, optimal alpha, jaccard, angular, manhattan, euclidean\n",
    "dataframe_dict = {'tasks' : ['author']*10 + ['subject']*10 + ['time period']*10 }\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "suffix = 'new_controlled'\n",
    "\n",
    "# Extracting the alpba=1 and optimal alpha column\n",
    "alpha1 = []\n",
    "alpha_opt = []\n",
    "# alpha1_weights = []\n",
    "# alpha_opt_weights = []\n",
    "\n",
    "for task in ['author', 'subject', 'time']:\n",
    "    task_numbers = [i for i in range(11,21)]\n",
    "\n",
    "    for num in task_numbers:\n",
    "        input_file_path_opt_alpha = f'../output_files/optimal_alpha_{suffix}_{task}{num}.pickle'\n",
    "        with open(input_file_path_opt_alpha, 'rb') as f:\n",
    "            opt_alpha_results = pickle.load(f)\n",
    "        \n",
    "        # input_file_path_opt_alpha_weights = f'../output_files/optimal_alpha_weights_{task}{num}.pickle'\n",
    "        # with open(input_file_path_opt_alpha_weights, 'rb') as f:\n",
    "        #     opt_alpha_results_weights = pickle.load(f)\n",
    "        \n",
    "        alpha1.append(round(opt_alpha_results[20],2))\n",
    "        alpha_opt.append(round(max(opt_alpha_results),2))\n",
    "        # alpha1_weights.append(round(opt_alpha_results_weights[20],2))\n",
    "        # alpha_opt.append(round(max(opt_alpha_results_weights),2))\n",
    "\n",
    "dataframe_dict['alpha=1'] = alpha1\n",
    "dataframe_dict['best alpha'] = alpha_opt\n",
    "# dataframe_dict['alpha=1 (weights)'] = alpha1_weights\n",
    "# dataframe_dict['best alpha (weights)'] = alpha_opt_weights\n",
    "\n",
    "\n",
    "# Frequency results \n",
    "for name in ['jaccard', 'euclidean_freq']:\n",
    "    input_file_path_current = f'../output_files/{name}_results_new.pickle'\n",
    "    with open(input_file_path_current, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    all_results = []\n",
    "    current_results = results_dict[suffix]\n",
    "    for key in current_results:\n",
    "        all_results += [round(i,2) for i in current_results[key]]\n",
    "    dataframe_dict[name] = all_results\n",
    "\n",
    "\n",
    "# Extracting the columns from embeddings\n",
    "for name in ['angular', 'manhattan', 'euclidean', 'euclidean_normed', 'jsd']:\n",
    "    input_file_path_current = f'../output_files/{name}_embedding_results_new.pickle'\n",
    "    with open(input_file_path_current, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    all_results = []\n",
    "    current_results = results_dict[suffix]\n",
    "    for key in current_results:\n",
    "        all_results += [round(i,2) for i in current_results[key]]\n",
    "    dataframe_dict[name] = all_results\n",
    "\n",
    "\n",
    "# Displaying as a dataframe\n",
    "df = pd.DataFrame(dataframe_dict)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary table of results (with average and SD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to above, but with average values and standard errors\n",
    "# Comparing results\n",
    "# Columns: alpha=1, optimal alpha, jaccard, angular, manhattan, euclidean\n",
    "dataframe_dict = {'tasks':['author', 'subject', 'time period']}\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "suffix = 'new'\n",
    "# suffix = 'new'\n",
    "\n",
    "# Extracting the alpba=1 and optimal alpha column\n",
    "alpha1 = []\n",
    "alpha_opt = []\n",
    "# alpha1_weights = []\n",
    "# alpha_opt_weights = []\n",
    "optimal_alphas = {'author':0.65, 'subject':0.6, 'time':0.8}\n",
    "opt_alpha_indexes = {'author':13, 'subject':12, 'time':16}\n",
    "\n",
    "for task in ['author', 'subject', 'time']:\n",
    "    task_numbers = [i for i in range(11,21)]\n",
    "    opt_alpha_index = opt_alpha_indexes[task]\n",
    "\n",
    "    current_alpha1 = []\n",
    "    current_alpha_opt = []\n",
    "    # current_alpha1_weights = []\n",
    "    # current_alpha_opt_weights = []\n",
    "\n",
    "    for num in task_numbers:\n",
    "        input_file_path_opt_alpha = f'../output_files/optimal_alpha_{suffix}_{task}{num}.pickle'\n",
    "        with open(input_file_path_opt_alpha, 'rb') as f:\n",
    "            opt_alpha_results = pickle.load(f)\n",
    "        \n",
    "        # input_file_path_opt_alpha_weights = f'../output_files/optimal_alpha_weights_{task}{num}.pickle'\n",
    "        # with open(input_file_path_opt_alpha_weights, 'rb') as f:\n",
    "        #     opt_alpha_results_weights = pickle.load(f)\n",
    "\n",
    "        current_alpha1.append(opt_alpha_results[20])\n",
    "        current_alpha_opt.append(opt_alpha_results[opt_alpha_index])\n",
    "        # current_alpha1_weights.append(opt_alpha_results_weights[20])\n",
    "        # current_alpha_opt.append(max(opt_alpha_results_weights))\n",
    "    \n",
    "    alpha1.append(f'{round(np.mean(np.array(current_alpha1)), 4)}' + \n",
    "                    ' \\u00B1 ' + f'{round(np.std(np.array(current_alpha1))/np.sqrt(10), 4)}')\n",
    "    alpha_opt.append(f'{round(np.mean(np.array(current_alpha_opt)), 4)}' + \n",
    "                    ' \\u00B1 ' + f'{round(np.std(np.array(current_alpha_opt))/np.sqrt(10), 4)}')\n",
    "    # alpha1_weights.append(f'{round(np.mean(np.array(alpha1_weights)), 2)}' + ' \\u00B1 ' + f'{round(np.std(np.array(alpha1_weights)), 2)}')\n",
    "    # alpha_opt_weights = f'{round(np.mean(np.array(alpha_opt_weights)), 2)}' + ' \\u00B1 ' + f'{round(np.std(np.array(alpha_opt_weights)), 2)}'\n",
    "\n",
    "dataframe_dict['alpha=1'] = alpha1\n",
    "dataframe_dict['best alpha'] = alpha_opt\n",
    "# dataframe_dict['alpha=1 (weights)'] = alpha1_weights\n",
    "# dataframe_dict['best alpha (weights)'] = alpha_opt_weights\n",
    "\n",
    "\n",
    "\n",
    "# Frequency results \n",
    "for name in ['jaccard', 'overlap', 'euclidean_freq', 'text_length']:\n",
    "    input_file_path_current = f'../output_files/{name}_results_new.pickle'\n",
    "    with open(input_file_path_current, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    all_results = []\n",
    "    current_results = results_dict[suffix]\n",
    "    for key in current_results:\n",
    "        all_results.append(f'{round(np.mean(np.array(current_results[key])), 4)}' + \n",
    "                            ' \\u00B1 ' + f'{round(np.std(np.array(current_results[key]))/np.sqrt(10), 4)}')\n",
    "    dataframe_dict[name] = all_results\n",
    "\n",
    "\n",
    "# Extracting the columns from embeddings\n",
    "for name in ['angular', 'manhattan', 'euclidean', 'euclidean_normed', 'jsd']:\n",
    "    input_file_path_current = f'../output_files/{name}_embedding_results_new.pickle'\n",
    "    with open(input_file_path_current, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    all_results = []\n",
    "    current_results = results_dict[suffix]\n",
    "    for key in current_results:\n",
    "        all_results.append(f'{round(np.mean(np.array(current_results[key])), 4)}' + \n",
    "                            ' \\u00B1 ' + f'{round(np.std(np.array(current_results[key]))/np.sqrt(10), 4)}')\n",
    "    dataframe_dict[name] = all_results\n",
    "\n",
    "# Displaying as a dataframe\n",
    "df = pd.DataFrame(dataframe_dict)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing against the baseline\n",
    "def compare_to_baseline(task, controlled, measure_name):\n",
    "    input_file_path_current = f'../output_files/{measure_name}_results_new.pickle'\n",
    "    with open(input_file_path_current, 'rb') as f:\n",
    "        results_dict = pickle.load(f)\n",
    "    all_results = []\n",
    "    current_results = results_dict[controlled][task]\n",
    "    return ttest_1samp(current_results, 0.5).pvalue\n",
    "\n",
    "for task in ['author','subject','time']:\n",
    "    for controlled in ['new']:\n",
    "        print(compare_to_baseline(task, controlled, 'text_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical test between alpha = 1 and optimal alpha\n",
    "optimal_alphas = {'author':0.65, 'subject':0.6, 'time':0.8}\n",
    "opt_alpha_indexes = {'author':13, 'subject':12, 'time':16}\n",
    "suffix = 'new'\n",
    "\n",
    "for task in ['author', 'subject', 'time']:\n",
    "    task_numbers = [i for i in range(11,21)]\n",
    "    opt_alpha_index = opt_alpha_indexes[task]\n",
    "\n",
    "    current_alpha1 = []\n",
    "    current_alpha_opt = []\n",
    "\n",
    "    for num in task_numbers:\n",
    "        input_file_path_opt_alpha = f'../output_files/optimal_alpha_{suffix}_{task}{num}.pickle'\n",
    "        with open(input_file_path_opt_alpha, 'rb') as f:\n",
    "            opt_alpha_results = pickle.load(f)\n",
    "\n",
    "        current_alpha1.append(opt_alpha_results[20])\n",
    "        current_alpha_opt.append(opt_alpha_results[opt_alpha_index])\n",
    "    \n",
    "    print(task, ttest_rel(current_alpha1, current_alpha_opt).pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests between optimal JSD and embedding\n",
    "optimal_alphas = {'author':0.65, 'subject':0.6, 'time':0.8}\n",
    "opt_alpha_indexes = {'author':13, 'subject':12, 'time':16}\n",
    "task = 'time'\n",
    "suffix = 'new'\n",
    "\n",
    "# Extracting JSD values\n",
    "task_numbers = [i for i in range(11,21)]\n",
    "opt_alpha_index = opt_alpha_indexes[task]\n",
    "\n",
    "current_alpha1 = []\n",
    "current_alpha_opt = []\n",
    "\n",
    "for num in task_numbers:\n",
    "    input_file_path_opt_alpha = f'../output_files/optimal_alpha_{suffix}_{task}{num}.pickle'\n",
    "    with open(input_file_path_opt_alpha, 'rb') as f:\n",
    "        opt_alpha_results = pickle.load(f)\n",
    "\n",
    "    current_alpha1.append(opt_alpha_results[20])\n",
    "    current_alpha_opt.append(opt_alpha_results[opt_alpha_index])\n",
    "\n",
    "\n",
    "# Getting embedding value\n",
    "measure_name = 'angular'\n",
    "with open(f'../output_files/{measure_name}_embedding_results_new.pickle', 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "emb_results = results_dict[suffix][task]\n",
    "\n",
    "# Getting Jaccard or overlap value\n",
    "name = 'jaccard'\n",
    "input_file_path_current = f'../output_files/{name}_results_new.pickle'\n",
    "with open(input_file_path_current, 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "vocab_results = results_dict[suffix][task]\n",
    "\n",
    "# result\n",
    "print(task, ttest_rel(vocab_results, current_alpha_opt).pvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output subject counts in descending order\n",
    "mq.reset()\n",
    "\n",
    "# Filter by language (English only)\n",
    "mq.filter_lang('en',how='only')\n",
    "\n",
    "# Only select books with more than 20 downloads\n",
    "df = mq.get_df()\n",
    "mq.df = df[df['downloads'] >= 20]\n",
    "\n",
    "# 1800 onwards\n",
    "mq.filter_year([1800, 2050])\n",
    "\n",
    "# Filter out data with no subject listed\n",
    "df = mq.get_df()\n",
    "mq.df = df[df['subjects'] != 'set()']\n",
    "\n",
    "# Filter out data that is missing author birth and death years\n",
    "df = mq.get_df()\n",
    "mq.df = df[df[['authoryearofbirth', 'authoryearofdeath']].notnull().all(1)]\n",
    "\n",
    "all_ids = mq.get_ids()\n",
    "text_lengths = []\n",
    "\n",
    "for id in all_ids:\n",
    "    try:\n",
    "        book = get_book(id)\n",
    "    except:\n",
    "        continue\n",
    "    length = sum(book.values())\n",
    "    text_lengths.append(length)\n",
    "\n",
    "print(np.mean(np.array(text_lengths)))\n",
    "print(np.std(np.array(text_lengths)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a word frequency plot of all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the full vocabulary\n",
    "all_ids = mq.get_ids()\n",
    "full_vocab = {}\n",
    "\n",
    "for id in all_ids:\n",
    "    try:\n",
    "        book = get_book(id)\n",
    "    except:\n",
    "        continue\n",
    "    for word in book:\n",
    "        if word in full_vocab:\n",
    "            full_vocab[word] += book[word]\n",
    "        else:\n",
    "            full_vocab[word] = book[word]\n",
    "\n",
    "# Store in a pickle file\n",
    "output_file_path = '../output_files/full_vocab.pickle'\n",
    "\n",
    "# with open(output_file_path, 'wb') as f:\n",
    "#     pickle.dump(full_vocab, f)\n",
    "\n",
    "# Rank-frequency plot of full vocabulary\n",
    "counts = []\n",
    "for word in full_vocab:\n",
    "    if full_vocab[word] > 0:\n",
    "        counts.append(full_vocab[word])\n",
    "counts.sort(reverse=True)\n",
    "freqs = np.array(counts)/sum(counts)\n",
    "ranks = [i for i in range(len(counts))]\n",
    "ranks.sort()\n",
    "\n",
    "plt.loglog(ranks, freqs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
